{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JSYda6A2IzDw"
   },
   "source": [
    "# Assignment 2: NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhjPl3PhJWOw"
   },
   "source": [
    "## Envirnment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ah-k_IkJbXR"
   },
   "source": [
    "Importing the dataset (needs to be done only once per notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jTX8EbBXKwrg"
   },
   "outputs": [],
   "source": [
    "# needs to be run only once per notebook\n",
    "# !wget \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "# !tar -xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2123,
     "status": "ok",
     "timestamp": 1522256273730,
     "user": {
      "displayName": "Abdelrahman Abdelnabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116891406932917050238"
     },
     "user_tz": -120
    },
    "id": "GoP_APVWSCcP",
    "outputId": "bed753c7-90f9-4f52-de4a-30e2fde351cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89526\r\n"
     ]
    }
   ],
   "source": [
    "!cat aclImdb/imdb.vocab | wc -l # number of vocab words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rksClKC7aN8b"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1522257452058,
     "user": {
      "displayName": "Abdelrahman Abdelnabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116891406932917050238"
     },
     "user_tz": -120
    },
    "id": "Ql-081v5aQbv",
    "outputId": "6391af35-0673-4ffa-962e-8b200299118f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import nltk # natural language tool kit: for text pre-processing\n",
    "import os # for listing directories\n",
    "from bs4 import BeautifulSoup as bs # library for removing html tags from text\n",
    "import numpy as np # no comment :P\n",
    "from nltk.corpus import stopwords # a set of common stopwords from nltk\n",
    "from gensim import models\n",
    "import gensim\n",
    "from collections import namedtuple\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from itertools import product\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from pathlib import Path\n",
    "# import helper functions\n",
    "from helpers import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1522257452058,
     "user": {
      "displayName": "Abdelrahman Abdelnabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116891406932917050238"
     },
     "user_tz": -120
    },
    "id": "Ql-081v5aQbv",
    "outputId": "6391af35-0673-4ffa-962e-8b200299118f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/abdelrahman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/abdelrahman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/abdelrahman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/abdelrahman/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download resources for nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1522257452058,
     "user": {
      "displayName": "Abdelrahman Abdelnabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116891406932917050238"
     },
     "user_tz": -120
    },
    "id": "Ql-081v5aQbv",
    "outputId": "6391af35-0673-4ffa-962e-8b200299118f"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', '``', \"''\", '...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZMnhAatZ9yz"
   },
   "source": [
    "## Reading the dataset and preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mV_QRyINaACa"
   },
   "outputs": [],
   "source": [
    "train_pos = []\n",
    "train_neg = []\n",
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 12500 positive training reviews\n",
      "read 12500 negative training reviews\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "train_pos = read_data('aclImdb/train/pos')\n",
    "print(\"read {} positive training reviews\".format(len(train_pos)))\n",
    "\n",
    "train_neg = read_data('aclImdb/train/neg')\n",
    "print(\"read {} negative training reviews\".format(len(train_neg)))\n",
    "\n",
    "train_data = train_pos + train_neg\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 12500 positive test reviews\n",
      "read 12500 negative test reviews\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "test_pos = read_data('aclImdb/test/pos')\n",
    "print(\"read {} positive test reviews\".format(len(test_pos)))\n",
    "\n",
    "test_neg = read_data('aclImdb/test/neg')\n",
    "print(\"read {} negative test reviews\".format(len(test_neg)))\n",
    "\n",
    "test_data = test_pos + test_neg\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array([1]*12500 + [0]*12500)\n",
    "test_labels = np.array([1]*12500 + [0]*12500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up hyper parameters and classifiers for cross fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the parameters to be tuned for each classifier and create a classifier object for each combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_params = {'C': [0.1, 0.5, 1, 5, 10]}\n",
    "random_forest_params = {'n_estimators': [10,50,100,300]}\n",
    "ada_params = {'n_estimators': [10, 50, 100], 'base_estimator':[LogisticRegression(C=5)]}\n",
    "knn_params = {'n_neighbors': [5, 11, 17]}\n",
    "\n",
    "lr_clfs = get_clfs_for_combinations(LogisticRegression, logistic_params)\n",
    "rf_clfs = get_clfs_for_combinations(RandomForestClassifier, random_forest_params)\n",
    "ada_clfs = get_clfs_for_combinations(AdaBoostClassifier, ada_params)\n",
    "knn_clfs = get_clfs_for_combinations(KNeighborsClassifier, knn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clf_scores(scores_dict, clf_params):\n",
    "    for clf in scores_dict.keys():\n",
    "        params = []\n",
    "        for param in clf_params:\n",
    "            params.append( (param, clf.get_params()[param]) )\n",
    "        print(\"clf with params: {}, score: {}\".format(params, scores_dict[clf]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Sp5oHDOUcojR"
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put positive and negative training data in one file\n",
    "!cat aclImdb/train/pos/all.txt aclImdb/train/neg/all.txt > all_train.txt\n",
    "!cat aclImdb/test/pos/all.txt aclImdb/test/neg/all.txt > all_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "all_train_file = open('all_train.txt', 'r')\n",
    "train_doc_matrix = tfidf.fit_transform([review for review in all_train_file.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67109\n"
     ]
    }
   ],
   "source": [
    "print(len(tfidf.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_file = open('all_test.txt', 'r')\n",
    "test_doc_matrix = tfidf.transform([review for review in all_test_file.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 67109)\n"
     ]
    }
   ],
   "source": [
    "print(test_doc_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf with params: [('C', 5)], score: 0.8911199999999999\n",
      "clf with params: [('C', 0.5)], score: 0.8810800000000001\n",
      "clf with params: [('C', 10)], score: 0.8907999999999999\n",
      "clf with params: [('C', 1)], score: 0.8872399999999999\n",
      "clf with params: [('C', 0.1)], score: 0.86012\n"
     ]
    }
   ],
   "source": [
    "scores_dict = cross_validate(train_doc_matrix, train_labels, lr_clfs)\n",
    "print_clf_scores(scores_dict, logistic_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get IDF values for each word in the vocabulary to use it in weighting the word2vec vectors later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idfs = dict(zip(tfidf.get_feature_names(), tfidf._tfidf.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec trained on IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dimensions = 100\n",
    "model=Word2Vec(train_data, size = word_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_vecs = get_doc_vecs_for_data(train_data, model.wv, word_dimensions, word_weights=word_idfs)\n",
    "test_doc_vecs = get_doc_vecs_for_data(test_data, model.wv, word_dimensions, word_weights=word_idfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation testing using IDF weighted embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = cross_validate(train_doc_vecs, train_labels, lr_clfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf with params: [('C', 10)], score: 0.80792\n",
      "clf with params: [('C', 1)], score: 0.80772\n",
      "clf with params: [('C', 0.1)], score: 0.8051600000000001\n",
      "clf with params: [('C', 5)], score: 0.8082800000000001\n",
      "clf with params: [('C', 0.5)], score: 0.80704\n"
     ]
    }
   ],
   "source": [
    "print_clf_scores(scores_dict, logistic_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict_300 = load_glove_dict(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_train_doc_vecs = get_doc_vecs_for_data(train_data, glove_dict_300, 300)\n",
    "glove_test_doc_vecs = get_doc_vecs_for_data(test_data, glove_dict_300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset_selective -f glove_dict_300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = cross_validate(glove_train_doc_vecs, train_labels, lr_clfs)\n",
    "print_clf_scores(scores_dict, logisitic_params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = cross_validate(glove_train_doc_vecs, train_labels, ada_clfs)\n",
    "print_clf_scores(scores_dict, ada_params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = cross_validate(glove_train_doc_vecs, train_labels, knn_clfs)\n",
    "print_clf_scores(scores_dict, knn_params.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on the Real Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = LogisticRegression()\n",
    "clf2.fit(glove_train_doc_vecs, train_labels)\n",
    "clf2.score(glove_test_doc_vecs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try using TF-IDF on the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileIterator:\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "    def __iter__(self):\n",
    "        pathlist = Path(self.dirname).glob('*_*.txt')\n",
    "        for path in pathlist:\n",
    "            path_in_str = str(path)\n",
    "            if os.path.isfile(path_in_str):\n",
    "                f = open(path_in_str)\n",
    "                yield f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_iter = FileIterator('aclImdb/train/pos')\n",
    "train_neg_iter = FileIterator('aclImdb/train/neg')\n",
    "train_iter = chain(train_pos_iter, train_neg_iter)\n",
    "\n",
    "test_pos_iter = FileIterator('aclImdb/test/pos')\n",
    "test_neg_iter = FileIterator('aclImdb/test/neg')\n",
    "test_iter = chain(test_pos_iter, test_neg_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "train_doc_matrix = tfidf.fit_transform(train_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_matrix = tfidf.transform(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf with params: [('C', 5)], score: 0.8973199999999999\n",
      "clf with params: [('C', 0.5)], score: 0.88128\n",
      "clf with params: [('C', 10)], score: 0.8974000000000002\n",
      "clf with params: [('C', 1)], score: 0.88892\n",
      "clf with params: [('C', 0.1)], score: 0.85152\n"
     ]
    }
   ],
   "source": [
    "scores_dict = cross_validate(train_doc_matrix, train_labels, lr_clfs)\n",
    "print_clf_scores(scores_dict, logistic_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "NLP.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
