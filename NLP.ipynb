{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PtsbmnJkeqh8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JSYda6A2IzDw"
   },
   "source": [
    "# Assignment 2: NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhjPl3PhJWOw"
   },
   "source": [
    "## Envirnment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ah-k_IkJbXR"
   },
   "source": [
    "Importing the dataset (needs to be done only once per notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jTX8EbBXKwrg"
   },
   "outputs": [],
   "source": [
    "# needs to be run only once per notebook\n",
    "# !wget \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "# !tar -xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2123,
     "status": "ok",
     "timestamp": 1522256273730,
     "user": {
      "displayName": "Abdelrahman Abdelnabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116891406932917050238"
     },
     "user_tz": -120
    },
    "id": "GoP_APVWSCcP",
    "outputId": "bed753c7-90f9-4f52-de4a-30e2fde351cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89526\r\n"
     ]
    }
   ],
   "source": [
    "!cat aclImdb/imdb.vocab | wc -l # number of vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xujKGM_cSXiP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rksClKC7aN8b"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1522257452058,
     "user": {
      "displayName": "Abdelrahman Abdelnabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116891406932917050238"
     },
     "user_tz": -120
    },
    "id": "Ql-081v5aQbv",
    "outputId": "6391af35-0673-4ffa-962e-8b200299118f"
   },
   "outputs": [],
   "source": [
    "import nltk # natural language tool kit: for text pre-processing\n",
    "import os # for listing directories\n",
    "from bs4 import BeautifulSoup as bs # library for removing html tags from text\n",
    "import numpy as np # no comment :P\n",
    "from nltk.corpus import stopwords # a set of common stopwords from nltk\n",
    "from gensim import models\n",
    "import gensim\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1522257452058,
     "user": {
      "displayName": "Abdelrahman Abdelnabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116891406932917050238"
     },
     "user_tz": -120
    },
    "id": "Ql-081v5aQbv",
    "outputId": "6391af35-0673-4ffa-962e-8b200299118f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/abdelrahman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/abdelrahman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/abdelrahman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/abdelrahman/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download resources for nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1522257452058,
     "user": {
      "displayName": "Abdelrahman Abdelnabi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116891406932917050238"
     },
     "user_tz": -120
    },
    "id": "Ql-081v5aQbv",
    "outputId": "6391af35-0673-4ffa-962e-8b200299118f"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}', '``', \"''\", '...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZMnhAatZ9yz"
   },
   "source": [
    "## Reading the dataset and preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "b3r_BWJ9lgJI"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    return get_wordnet_pos(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mV_QRyINaACa"
   },
   "outputs": [],
   "source": [
    "def read_data(data_set_path):\n",
    "    data = []\n",
    "    \n",
    "    if any(\"all.txt\" in s for s in os.listdir(data_set_path)):\n",
    "        # read the reviews line by line\n",
    "        all_data_file_path = os.path.join(data_set_path, 'all.txt')\n",
    "        all_data_file = open(all_data_file_path, 'r')\n",
    "        for line in all_data_file.readlines():\n",
    "            vec = line.split(\" \")\n",
    "            vec = [item for item in vec if item != '\\n']\n",
    "            data.append(vec)\n",
    "        return data\n",
    "        \n",
    "    \n",
    "    for file_name in os.listdir(data_set_path):\n",
    "        file_path = os.path.join(data_set_path, file_name)\n",
    "        file = open(file_path, 'r')\n",
    "\n",
    "        # read raw text\n",
    "        text = file.read()\n",
    "\n",
    "        # remove html tags\n",
    "        text = bs(text, \"html.parser\").get_text()\n",
    "\n",
    "        # tokenize the text\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # get the part of speech tags for the tokens\n",
    "        # This gives each token a tag from [NOUN, VERB, ADJ, ADV] which are\n",
    "        # used by the lemmatizer to correctly lemmatize a word\n",
    "        tagged_text = nltk.pos_tag(tokens)\n",
    "\n",
    "        # lowercase, remove stop words, and lemmatize\n",
    "        tokens = [wnl.lemmatize(tok.lower(), penn_to_wn(tag)) for (tok, tag) in tagged_text if tok.lower() not in stop_words and not tok.isdigit()]\n",
    "        data.append(tokens)\n",
    "    \n",
    "    all_data_file_path = os.path.join(data_set_path, 'all.txt')\n",
    "    all_data_file = open(all_data_file_path, 'w+')\n",
    "    all_data_file.writelines([\"%s\\n\" % \" \".join(str(x) for x in item) for item in data])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mV_QRyINaACa"
   },
   "outputs": [],
   "source": [
    "train_pos = []\n",
    "train_neg = []\n",
    "\n",
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 12500 positive training reviews\n",
      "read 12500 negative training reviews\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "train_pos = read_data('aclImdb/train/pos')\n",
    "print(\"read {} positive training reviews\".format(len(train_pos)))\n",
    "\n",
    "train_neg = read_data('aclImdb/train/neg')\n",
    "print(\"read {} negative training reviews\".format(len(train_neg)))\n",
    "\n",
    "train_data = train_pos + train_neg\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 12500 positive test reviews\n",
      "read 12500 negative test reviews\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "test_pos = read_data('aclImdb/test/pos')\n",
    "print(\"read {} positive test reviews\".format(len(test_pos)))\n",
    "\n",
    "test_neg = read_data('aclImdb/test/neg')\n",
    "print(\"read {} negative test reviews\".format(len(test_neg)))\n",
    "\n",
    "test_data = test_pos + test_neg\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Sp5oHDOUcojR"
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put positive and negative training data in one file\n",
    "!cat aclImdb/train/pos/all.txt aclImdb/train/neg/all.txt > all_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x67109 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2408630 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "all_train_file = open('all_train.txt', 'r')\n",
    "tfidf.fit_transform([review for review in all_train_file.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 67109)\n"
     ]
    }
   ],
   "source": [
    "print(doc_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idfs = dict(zip(tfidf.get_feature_names(), tfidf._tfidf.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dimensions = 100\n",
    "\n",
    "model=Word2Vec(train_data, size = word_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.1617316 , -1.1810275 ,  1.7089483 , -1.4388052 , -0.9129895 ,\n",
       "       -0.19307204,  1.4702221 , -0.40685564, -0.18956885, -0.01083623,\n",
       "       -0.7461476 , -0.08285325, -1.3399551 , -0.7335147 , -1.0785775 ,\n",
       "        0.37544942, -0.40843806,  0.5222615 ,  0.65392613,  1.4214958 ,\n",
       "       -0.46880654,  1.6329281 ,  0.30859506, -0.61773735, -0.41692367,\n",
       "       -0.42304382, -0.8085791 , -0.3581976 , -0.4381851 ,  1.4749767 ,\n",
       "       -1.1013819 , -1.0666571 , -0.89562565,  0.70554197,  0.42756954,\n",
       "        0.99748874, -0.8049225 ,  0.10262369, -0.7526793 , -0.33935434,\n",
       "       -0.5148289 , -0.0940445 ,  0.31421944, -0.02793522,  0.5785908 ,\n",
       "       -0.34037593,  0.33859348, -0.8858172 ,  0.11720247,  0.34867325,\n",
       "        0.03178314, -0.07964987, -0.24286842, -0.03939491,  0.01047644,\n",
       "       -0.99116737,  1.3539529 , -0.03405824, -0.41418836, -0.25336492,\n",
       "       -0.40225163, -0.0407508 , -0.03256291, -0.2230316 , -0.6170128 ,\n",
       "       -0.34822702,  0.37247574,  0.26393715, -1.3389213 , -1.4575012 ,\n",
       "        0.78998774,  0.70945966,  0.82674396, -1.050449  , -0.0019359 ,\n",
       "        1.4303977 ,  0.08038843, -0.52539176, -0.9186761 ,  1.7995394 ,\n",
       "       -1.5482321 ,  0.53765774,  0.23117876,  0.03403153, -0.39759842,\n",
       "        0.81928307,  0.8776526 ,  0.1203243 ,  0.89715356,  1.7488272 ,\n",
       "        0.345737  , -0.3471178 , -0.42004576,  1.1405377 , -0.7544981 ,\n",
       "       -0.4132883 , -0.8783999 , -0.15746386,  0.5643929 ,  0.48711866],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.wv['cartoon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vecs_for_data(data):\n",
    "    doc_vecs = np.empty([len(data), word_dimensions])\n",
    "\n",
    "    for idx,review in enumerate(data):\n",
    "        doc_vec = np.zeros([1, word_dimensions])\n",
    "        for word in review:\n",
    "            if word in model.wv and word in word_idfs:\n",
    "                doc_vec += word_idfs[word]*model.wv[word]\n",
    "        doc_vecs[idx] = doc_vec\n",
    "    \n",
    "    return doc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_vecs = get_doc_vecs_for_data(train_data)\n",
    "test_doc_vecs = get_doc_vecs_for_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying using TF-IDF weighted embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.809"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(train_doc_vecs, [1]*12500 + [0]*12500)\n",
    "clf.score(test_doc_vecs, [1]*12500 + [0]*12500)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "NLP.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
